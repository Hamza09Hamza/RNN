{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Mod√©lisation\n",
    "\n",
    "Dans cette section, nous allons entamer la phase de mod√©lisation, qui consiste √† :\n",
    "\n",
    "- Choisir un ou plusieurs mod√®les adapt√©s au type de probl√®me (classification ou r√©gression),\n",
    "- Entra√Æner ces mod√®les sur l‚Äôensemble d‚Äôapprentissage,\n",
    "- Optimiser leurs performances √† l‚Äôaide de la validation crois√©e et de la recherche d‚Äôhyperparam√®tres (GridSearchCV, RandomizedSearchCV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Path_Data='../Data/Processed/'\n",
    "X_train = pd.read_csv(Path_Data+'X_train.csv')\n",
    "X_val = pd.read_csv(Path_Data+'X_val.csv')\n",
    "X_test = pd.read_csv(Path_Data+'X_test.csv')\n",
    "\n",
    "y_train = pd.read_csv(Path_Data+'y_train.csv')\n",
    "y_val = pd.read_csv(Path_Data+'y_val.csv')\n",
    "y_test = pd.read_csv(Path_Data+'y_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation des Mod√®les \n",
    "\n",
    "Nous allons utiliser les mod√®les suivants pour nos t√¢ches de classification et r√©gression :\n",
    "\n",
    "- **CatBoost**\n",
    "- **LightGBM (LGBM)**\n",
    "- **XGBoost**\n",
    "- **For√™t Al√©atoire (Random Forest)**\n",
    "- **KNN (K-Nearest Neighbors)**\n",
    "- Et d'autres mod√®les selon les besoins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from LogisticRegression import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On definie les Grids de Parametres Pour chaque model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'alpha': [0.001, 0.01, 0.1],  \n",
    "        'iterations': [500, 1000, 2000],  \n",
    "        'use_l2': [True, False],  \n",
    "        'lambda_': [0.01, 0.1, 1.0],  # L2 regularization strength\n",
    "        'use_decay': [True, False],\n",
    "        'decay': [0.001, 0.01, 0.1],\n",
    "        'early_stopping': [True, False],\n",
    "        'tol': [1e-4, 1e-5] \n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        'iterations': [500, 1000],\n",
    "        'depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        'num_leaves': [31, 50, 100],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'n_neighbors': [3, 5, 7, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  D√©tection de l‚Äôenvironnement d'ex√©cution (CPU, GPU NVIDIA, ou puce Apple M1/M2..)\n",
    "\n",
    "Ce script Python permet de **d√©tecter automatiquement** l'environnement mat√©riel sur lequel votre code est ex√©cut√©, afin d‚Äôadapter l'entra√Ænement des mod√®les (par exemple : activer l‚Äôutilisation du GPU quand c‚Äôest possible).\n",
    "Dans Notre Cas on a deux Puissante Machine l'une avec M1 Pro  et l'autre avec une Carte Graphique RTX 4070  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def detect_environment():\n",
    "    system = platform.system().lower()\n",
    "    machine = platform.machine().lower()\n",
    "\n",
    "    # Apple Silicons (M1/M2)\n",
    "    if system == 'darwin' and 'arm' in machine:\n",
    "        return \"M1\"\n",
    "\n",
    "    # CUDA-compatible NVIDIA GPU\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode == 0:\n",
    "            return \"CUDA\"\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return \"CPU\"\n",
    "\n",
    "env = detect_environment()\n",
    "print(f\" Environnement d√©tect√© : {env}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Optimisation des hyperparam√®tres avec GridSearchCV et acc√©l√©ration GPU\n",
    "\n",
    "Pour garantir les meilleures performances de chaque mod√®le de classification, nous utilisons **GridSearchCV** pour effectuer un r√©glage fin des hyperparam√®tres. Voici les √©tapes :\n",
    "\n",
    "1. **Exclusion des mod√®les personnalis√©s** :\n",
    "   - Le mod√®le de r√©gression logistique impl√©ment√© manuellement est exclu car il ne prend pas en charge `GridSearchCV` directement.\n",
    "\n",
    "2. **Utilisation compl√®te du CPU et du GPU** :\n",
    "   - `n_jobs = -1` permet d‚Äôutiliser tous les c≈ìurs du processeur pour les calculs parall√®les.\n",
    "   - Pour les mod√®les compatibles avec le GPU, nous activons explicitement l'acc√©l√©ration :\n",
    "     - **XGBoost** : `tree_method='gpu_hist'`\n",
    "     - **LightGBM** : `device='gpu'`\n",
    "     - **CatBoost** : `task_type='GPU'`, `devices='0'`\n",
    "\n",
    "3. **Affichage d√©taill√© de l'entra√Ænement** :\n",
    "   - `verbose=2` affiche les √©tapes d√©taill√©es de l'entra√Ænement, ce qui permet de suivre la progression en temps r√©el.\n",
    "\n",
    "4. **Validation crois√©e** :\n",
    "   - Une validation crois√©e √† 3 plis (`cv=3`) est utilis√©e pour √©viter le surapprentissage et am√©liorer la robustesse de la s√©lection des mod√®les.\n",
    "\n",
    "5. **√âvaluation** :\n",
    "   - Apr√®s l'entra√Ænement, nous extrayons les meilleurs hyperparam√®tres et √©valuons le mod√®le sur l'ensemble de validation √† l‚Äôaide de la pr√©cision (`accuracy`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n D√©marrage de la GridSearch pour : {model_name}\")\n",
    "\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        print(\"  GridSearch ignor√©e pour la r√©gression logistique personnalis√©e.\")\n",
    "        continue\n",
    "\n",
    "    # Appliquer les bons param√®tres selon l‚Äôenvironnement\n",
    "    if env == \"CUDA\":\n",
    "        if model_name == \"XGBoost\":\n",
    "            model.set_params(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        elif model_name == \"LightGBM\":\n",
    "            model.set_params(device='gpu')\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model.set_params(task_type='GPU', devices='0', verbose=0)\n",
    "\n",
    "    elif env == \"M1\":\n",
    "        if model_name == \"LightGBM\":\n",
    "            model.set_params(device_type='gpu')  # Metal backend\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model.set_params(task_type='CPU', verbose=0)\n",
    "\n",
    "    else:  # CPU fallback\n",
    "        if model_name == \"XGBoost\":\n",
    "            model.set_params(tree_method='hist', predictor='cpu_predictor')\n",
    "        elif model_name == \"LightGBM\":\n",
    "            model.set_params(device_type='cpu')\n",
    "        elif model_name == \"CatBoost\":\n",
    "            model.set_params(task_type='CPU', verbose=0)\n",
    "\n",
    "    # Lancer GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grids[model_name],\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\" Meilleurs hyperparam√®tres pour {model_name} : {grid_search.best_params_}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_val = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred_val)\n",
    "    print(f\" {model_name} - Accuracy Validation : {val_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
